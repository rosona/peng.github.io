<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Rosona&#39;s Blog</title>
    <link>https://rosona.github.io/post/</link>
    <description>Recent content in Posts on Rosona&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 27 Dec 2015 13:42:46 +0800</lastBuildDate>
    <atom:link href="https://rosona.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>服务器性能工具箱 （1）－ 网络带宽</title>
      <link>https://rosona.github.io/post/20151227/</link>
      <pubDate>Sun, 27 Dec 2015 13:42:46 +0800</pubDate>
      
      <guid>https://rosona.github.io/post/20151227/</guid>
      <description>&lt;p&gt;工欲善其事必先利其器，服务器端性能工具箱从“器”出发，本系列文章介绍我遇到的听到的一些关于性能优化的故事&lt;/p&gt;

&lt;p&gt;有时候客户端出现访问慢无法访问偶尔无法打开，有很大的延迟丢包现象，我们查遍了服务器端组件，web service，database，服务器负载，问题仍没有解决，这时候你可能需要考虑一下网络带宽
​
怎么查看你的带宽是否有问题，介绍两个网络带宽相关的工具：nload，ethstatus，下面以nload为例，ethstatus安装及使用方式可以自行搜索&lt;/p&gt;

&lt;p&gt;在Ubuntu下安装非常简单：sudo apt-get install nload
安装后使用：nload eth1，eth1为你想检测的网络接口，会出现如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rosona.github.io/img/nload_example.jpg&#34; alt=&#34;微信公众号&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上面是所监测接口进入的流量，下面是所监测接口出去的流量，用＃现实的流量图会实时变化，有当前流量，平均流量，最小流量，最大流量，综合流量
和从服务商协议流量做对比，如果已经达到的最大值，需要关注了&lt;/p&gt;

&lt;p&gt;确定是流量问题后，接下来需要看是什么导致流量增加了，有几种情况：
正常的业务增长，这是好事！
是否上了耗费流量的业务？服务器上host了image service？有一个推广的视频？如果是，可以考虑CDN等
静态资源版本使用问题？我们产品最初遇到过每一次版本升级后流量规律性的有一个峰值，由于静态资源（js等）版本未正确使用，每次升级都重新获取所有资源，集中的升级导致流量激增
DDOS？你摊上事儿了，准备接招吧，留意邮件！（后面我们会有一遍文章单独分析）&lt;/p&gt;

&lt;p&gt;如果确定是业务增长所致，那真是快乐的事情，赶紧增加带宽吧，其它情况如果不能尽快解决，建议先增加带宽，保证用户体验，然后尽快真对具体问题做优化处理&lt;/p&gt;

&lt;p&gt;当然，现在我们一般都使用云服务，比如aws，aliyun等，云服务会提供非常详细的带宽分析图，比如aws的CloudWatch，可以配合nload使用（并不是每一个dev都有console管理权限）。国内aliyun是预付费，带宽有按需（用多少付多少）也有固定带宽，一般我们都是选择固定带宽，避免高额费用产生（国内的带宽费用非常昂贵），aws的带宽是按需使用&lt;/p&gt;

&lt;p&gt;除了云服务商提供的简单的带宽分析之外，我们要想对业务全面了如指掌，需要对服务的VPC做更加详细的监控，了解出入口流量，业务模块间的数据流动很重要，这时候我们可以使用第三方的监控工具协助，比如zabbix，具体怎么使用自行解决，此处不做描述&lt;/p&gt;

&lt;p&gt;清楚了VPC内部及外部的流量情况以及支撑的最大值，我们就能针对实际的业务模块做调整及优化&lt;/p&gt;

&lt;p&gt;后面我会持续更新性能工具箱，一次专注描述一个点，请保持关注&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rosona.github.io/img/qrcode_for_beluga.jpg&#34; alt=&#34;微信公众号&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>浅谈流处理算法 (1) – 蓄水池采样</title>
      <link>https://rosona.github.io/post/20151223/</link>
      <pubDate>Wed, 23 Dec 2015 23:50:53 +0800</pubDate>
      
      <guid>https://rosona.github.io/post/20151223/</guid>
      <description>

&lt;p&gt;前言
现如今，“大数据 ”已经不是什么新概念，“一千个人眼中有一千个大数据”。社交网络，智能穿戴设备，智能家居，传感器，机器人等每一个热门的词汇背后都是大量的数据。抛开各种噱头和概念，相信每个人都能看到数据的价值，且能感受到数据规模的爆炸式增长。大规模的数据本身并不产生什么价值，只有通过理解数据，发现知识，避免“Garbage In Garbage Out” 才能发挥数据的价值。&lt;/p&gt;

&lt;p&gt;在形式多样的大数据问题中，有一类问题输入以数据流的方式提供。我们需要在有限空间内通过若干遍访问数据流(很多时候可能仅仅允许访问一遍)完成数据处理并提取有价值的信息。我们称处理这类问题的算法为“流处理算法”(Streaming Algorithm)&lt;/p&gt;

&lt;p&gt;Wikipedia定义：
In computer science, streaming algorithms are algorithms for processing data streams in which the input is presented as a sequence of items and can be examined in only a few passes (typically just one). These algorithms have limited memory available to them (much less than the input size) and also limited processing time per item.&lt;/p&gt;

&lt;p&gt;一些典型的例子：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;根据用户访问日志，统计用户来源分布？&lt;/li&gt;
&lt;li&gt;根据用户访问日志，统计新增用户数目？&lt;/li&gt;
&lt;li&gt;根据用户访问日志，统计Unique User数目？&lt;/li&gt;
&lt;li&gt;根据用户访问日志，统计某个页面访问次数？&lt;/li&gt;
&lt;li&gt;&amp;hellip;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些问题在数据规模并不大且事先确定的情况下，采取直接的统计方案并不难实现。但，在数据流场景下，直接的方案并不能满足实际需求。&lt;/p&gt;

&lt;p&gt;数据流问题有以下特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据规模大&lt;/li&gt;
&lt;li&gt;存储空间有限。保存完整的数据集合并不现实。&lt;/li&gt;
&lt;li&gt;分发速度快。如果处理不及时，没有足够资源保存数据。&lt;/li&gt;
&lt;li&gt;允许数据遍历极少次数，一般仅支持遍历一次。&lt;/li&gt;
&lt;li&gt;空间复杂度sub-linear&lt;/li&gt;
&lt;li&gt;时间复杂度linear&lt;/li&gt;
&lt;li&gt;可以接受近似解&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;曾几何时，拜摩尔定律和硬件技术所赐，软件开发脱离在64k内存中扣扣索索的日子，进入没事开个大数组的印度模式时代(道听途说，如有雷同，概不负责)。软件体量一个比一个吓人。然而，面对上述特点的数据流问题时，我们再一次深刻地体会到日新月异的计算机技术背后总有一个背景音 - “New Wine In Old Bottles”。&lt;/p&gt;

&lt;p&gt;数据库大牛Jim Gray多年前就指示我们：Tape is Dead, Disk is Tape, Flash is Disk, RAM locality is King.
TalkingData的Bitmap引擎这不又在扣扣索索了嘛&lt;/p&gt;

&lt;p&gt;这个小系列，就是跟大家一起品尝一下“新酒”， 以酒会友:-)&lt;/p&gt;

&lt;p&gt;言归正传，针对上述数据流问题特点，流处理算法的一般模式有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;采样/过滤 – 减少规模

&lt;ul&gt;
&lt;li&gt;如：蓄水池采样 Reservoir Sampling&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;随机映射(Random Projection) – 降维

&lt;ul&gt;
&lt;li&gt;Hash/SimHash&lt;/li&gt;
&lt;li&gt;概要 (Sketch)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;统计直方图(Histogram)

&lt;ul&gt;
&lt;li&gt;滑动窗口&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些算法模式都是希望通过样本采样或者特征采样，使用有限数据保持原整体数据的某些特性。&lt;/p&gt;

&lt;p&gt;后续我们计划聊聊蓄水池采样， 存在性查询，基数估计，频率估计，频繁项估计等问题的流处理方法。&lt;/p&gt;

&lt;h2 id=&#34;蓄水池采样:5463a92c58271dbff34bfff1d5fdac02&#34;&gt;蓄水池采样&lt;/h2&gt;

&lt;p&gt;蓄水池采样是一类采样算法的总称，广泛使用在从数据流中按一定需求抽样出部分样本的场景中。蓄水池，可以看作小段缓存，协助存储数据流的部分信息。&lt;/p&gt;

&lt;p&gt;下面我们从三个具体例子看看什么是蓄水池采样。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;问题1. 年度大奖将从参加年会的同事中随机选择，如何维护一个获奖者名字，不管何时老大宣布获奖者，所有参会者获奖概率一致？&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;假如老大宣布获奖者时，参会同事人数为N，我们需要保证每个参会者获奖概率为1/N。&lt;/p&gt;

&lt;p&gt;蓄水池采样算法是这样的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;记录第一个到达的同学到获奖者名单上 (早起的鸟儿有虫吃 :-) )&lt;/li&gt;
&lt;li&gt;当第i位同学到达年会会场的时候(i&amp;gt;1):

&lt;ul&gt;
&lt;li&gt;以1/i的概率使用第i位同学替换获奖者名单上的同学&lt;/li&gt;
&lt;li&gt;以1-1/i的概率保持获奖者名单不变 (Yeah!守擂成功!)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;看起来很简单，我们来看看是否能满足问题的需求&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一个同学P_1到达时，获奖者必定是P_1。(获奖概率 &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;1&lt;/sub&gt; = 100%，满足)&lt;/li&gt;
&lt;li&gt;第二个同学P_2到达时，1/2概率获奖者替换为P_2, 也1/2概率保持为P_1 (满足)&lt;/li&gt;
&lt;li&gt;当第i位同学到达时，1/i的概率获奖者替换为P_i。而的1-1/i概率保持获奖者不变，也就是P_1, P&lt;em&gt;2, &amp;hellip;, P&lt;/em&gt;(i-1)获奖的概率为 1/(i-1) * (1-1/i) = 1/i (满足)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;综上所述，根据归纳法，蓄水池采样搞掂了。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;问题2. 项目进展很好，老大很高兴，决定年度大奖有k名，如何维护一个k个获奖者名单，保证不管何时老大宣布获奖者，所有参会者获奖概率一致？&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;假如老大宣布获奖者时，参会同事人数为N，我们需要保证每个参会者获奖概率为k/N。&lt;/p&gt;

&lt;p&gt;蓄水池采样算法是这样的：
  + 记录前k位到达的同学到获奖者名单上 (早起总没错的)
  + 当第i位同学到达年会会场的时候(i&amp;gt;k):
    - 以k/i的概率使用第i位同学替换获奖者名单上的随机一位同学(某位同学杯具了)
    - 以1-k/i的概率保持获奖者名单不变 (集体守擂成功!)&lt;/p&gt;

&lt;p&gt;看起来跟上一个问题差不多，应该是正确的吧&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;前k位同学到达时，获奖者必定是P_1, P_2, &amp;hellip;, P_k。(获奖概率 k/k = 100%，满足)&lt;/li&gt;
&lt;li&gt;当第i位同学到达时( i &amp;gt; k)

&lt;ul&gt;
&lt;li&gt;[Condition 1] k/i概率获奖者替换为P_i (满足)&lt;/li&gt;
&lt;li&gt;[Condition 2] 1 - k/i的概率保持获奖者名单不变&lt;/li&gt;
&lt;li&gt;对于P_x (x &amp;lt; i), 在第i位同学尚未到达时获奖概率为k/(i-1)。她能保留在获奖名单中的概率由两部分组成 k/(i-1) * [k/i * (1 - 1/k) + 1 - k/i] = k/i(满足)

&lt;ul&gt;
&lt;li&gt;[Condition 1]没有被P_i踢掉 k/i * (1 - 1/k)&lt;/li&gt;
&lt;li&gt;[Condition 2]P_i直接出局了1 - k/i&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;综上所述，根据归纳法，蓄水池采样又搞掂了。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;问题3.为了激励绩效好的同学，每个参会同学都有一个权重，如何维护一个k个获奖者名单，保证不管何时老大宣布获奖者，所有参会者获奖概率与权重成比例？&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;假如老大宣布获奖者时，参会同事人数为N，我们需要保证参会者(P&lt;em&gt;, 绩效权重W&lt;/em&gt;)获奖概率为W_i/(W_1 + W_2 + &amp;hellip; + W_N)。&lt;/p&gt;

&lt;p&gt;蓄水池采样算法是这样的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;记录前k位到达的同学到获奖者名单上 (嗯，早起永远没错的)

&lt;ul&gt;
&lt;li&gt;每位同学的分值S_i = random(0, 1) ^ (1/W_i)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;当第i位同学到达年会会场的时候(i&amp;gt;k):

&lt;ul&gt;
&lt;li&gt;计算第位同学的分值 S_i = random(0, 1) ^ (1/W_i)&lt;/li&gt;
&lt;li&gt;如果获奖者名单中分值最小的同学P_x的分值S_x比S_i小，P_x被S_i替换了&lt;/li&gt;
&lt;li&gt;否则，获奖者名单不变 (集体守擂成功!)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;怎么证明这样可以满足要求呢？额，“这里空白太小，我写不下了” (自己看论文去吧)&lt;/p&gt;

&lt;p&gt;年会一年也就一次，很快来了又走了。各种各样的数据流每天都在产生，这些蓄水池采样方法可以帮忙我们获得数据流的一个样本。对于小规模样本可以保持大规模数据信息的场景，蓄水池采样帮忙我们把“大数据”的“大”给去掉了。&lt;/p&gt;

&lt;p&gt;参考文献&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Streaming_algorithm&#34;&gt;https://en.wikipedia.org/wiki/Streaming_algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Reservoir_sampling&#34;&gt;https://en.wikipedia.org/wiki/Reservoir_sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vitter, Jeffrey S. (1 March 1985). &amp;ldquo;Random sampling with a reservoir&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Efraimidis, Pavlos S.; Spirakis, Paul G. (2006-03-16). &amp;ldquo;Weighted random sampling with a reservoir&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>最牛逼的开源机器学习框架，你知道几个</title>
      <link>https://rosona.github.io/post/20151212/</link>
      <pubDate>Sat, 12 Dec 2015 23:42:53 +0800</pubDate>
      
      <guid>https://rosona.github.io/post/20151212/</guid>
      <description>&lt;p&gt;机器学习毫无疑问是当今最热的话题，它已经渗透到生活的方方面面，在移动互联网中混不懂点机器学习都不好意思，说几个能看的到的，经常用邮箱吧，是不是感觉垃圾邮件比N年前变少了，无聊了和siri聊过天不，想坐一下无人驾驶汽车吗，手累了用脸解个锁，智能化产品推荐是不是让你更懒了。看不到的就更多了：信用卡欺诈监测保证你的交易安全，股票交易/量化投资（知道你的高收益理财怎么来的吗？），手势识别（用过海豚浏览器的手势吗），还有医学分析等等，巨头们为了在未来占领先机，前仆后继的开源他们的机器学习框架，加速了人类进入智能时代的步伐（说什么，机器人？）&lt;/p&gt;

&lt;p&gt;Facebook：用于Torch的模块库fbcunn （2015-01-17 开源）
fbcunn可以替代Torch的默认模块，它们构建在Nvidia的cuFFT库（一个基于CUDA的库，用于深度神经网络）之上，可以在更短的时间内训练更大规模的神经网络模型，它对NVIDIA的GPU进行了优化。一部分可以用来训练大型计算机视觉系统。部分模块也可以用来训练处理不同类型数据的模型。既可以进行文本识别、图像识别，也能用于语言模型的训练。部分模块将大型卷积神经网络模型的训练速度提升了23.5倍。
fbcunn基于Fast Training of Convolutional Networks through FFTs这篇论文中的想法构建了这些模块，FAIR（Facebook人工智能实验室）的主任Yann LeCun是论文的合著者之一。与cuDNN相比，在卷积核较小的情况下（3x3），fbcunn的速度提升可达1.84倍；而在卷积核较大的情况下（5x5），速度提升可达23.5倍。
Torch和fbcunn的最早的用途之一：图片分类，它分类过ImageNet的120万张图片，可以参考这个地址：&lt;a href=&#34;https://github.com/soumith/imagenet-multiGPU.torch&#34;&gt;https://github.com/soumith/imagenet-multiGPU.torch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;参考：
&lt;a href=&#34;http://torch.ch&#34;&gt;http://torch.ch&lt;/a&gt;
&lt;a href=&#34;https://github.com/torch/&#34;&gt;https://github.com/torch/&lt;/a&gt;
&lt;a href=&#34;https://github.com/facebook/fbcunn&#34;&gt;https://github.com/facebook/fbcunn&lt;/a&gt;
&lt;a href=&#34;https://research.facebook.com/blog/879898285375829/fair-open-sources-deep-learning-modules-for-torch/&#34;&gt;https://research.facebook.com/blog/879898285375829/fair-open-sources-deep-learning-modules-for-torch/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;微软：DMTK（2015-11-16 开源）
DMTK由参数服务器和客户端SDK两部分构成。参数服务器支持存储混合数据结构模型、接受并聚合工作节点服务器的数据模型更新、控制模型同步逻辑；客户端SDK负责维护节点模型缓存（与全局模型服务器同步）、本地训练和模型通讯之间的流水线控制以及片状调度大模型训练。它包含DMTK框架、LightLDA和分布式词向量（Word Embedding）三个组件。
DMTK采用了传统的客户端/服务器架构，有多个服务器实例运行在多台机器上负责维护全局模型参数，而训练例程（routines）则使用客户端API访问并更新这些参数。为了适应不同的集群环境，DMTK框架支持两种进程间的通信机制：MPI和ZMQ。应用程序端不需要修改任何代码就能够在这两种方式之间切换。DMTK支持Windows和Linux两种操作系统。
DMTK则是使用C++编写的，提供了一个客户端API和SDK。 DMTK的官网 对DMTK框架、LightLDA、分布式词向量的应用场景、下载、安装、配置、运行以及性能等方面都做了详尽的介绍（见参考部分）。
DMTK主要用于自然语言处理方面，比如：文本分类与聚类、话题识别以及情感分析等&lt;/p&gt;

&lt;p&gt;参考：
&lt;a href=&#34;http://www.dmtk.io&#34;&gt;http://www.dmtk.io&lt;/a&gt;
&lt;a href=&#34;https://github.com/Microsoft/DMTK&#34;&gt;https://github.com/Microsoft/DMTK&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Google：TensorFlow（2015-11-10 开源）
TensorFlow 是一个用来编写和执行机器学习算法的工具。计算在数据流图中完成，图中的节点进行数学运算，边界是在各个节点中交换的张量（Tensors&amp;ndash;多维数组）。TensorFlow负责在不同的设备、内核以及线程上异步地执行代码，目前支持CNN、RNN和LSTM等图像、语音和自然语言处理（NLP）领域最流行的深度神经网络模型。
Google已将TensorFlow用于GMail（SmartReply）、搜索（RankBrain）、图片（生成图像分类模型&amp;ndash;Inception Image Classification Model）、翻译器（字符识别）等产品。
TensorFlow能够在台式机、服务器或者移动设备的CPU和GPU上运行，也可以使用Docker容器部署到云环境中。在处理图像识别、语音识别和语言翻译等任务时，TensorFlow依赖于配备图像处理单元（GPU）的机器和被用于渲染游戏图像的芯片，它对这些芯片依赖度比想象中的高。当前开源的版本能够运行在单机上，暂不支持集群。操作系统方面，TensorFlow能够运行在Linux和MacOS上。
TensorFlow的核心是使用C++编写的，有完整的Python API和C++接口，同时还有一个基于C的客户端API。
参考：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org&#34;&gt;https://www.tensorflow.org&lt;/a&gt;
&lt;a href=&#34;https://github.com/tensorflow/tensorflow&#34;&gt;https://github.com/tensorflow/tensorflow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;IBM：SystemML （2015-06 开源）
SystemML是灵活的，可伸缩机器学习(ML) 语言，使用Java编写。可实现 可定制算法（述性分析、分类、聚类、回归、矩阵分解及生存分析等）， 多个执行模式（单独运行、Hadoop 和 Spark ）， 自动优化。它由 IBM 的 Almaden 实验室花了近 10年开发而成的机器学习技术。
SystemML语言，声明式机器学习 (DML)。SystemML 包含线性代数原语，统计功能和 ML 指定结构，可以更容易也更原生的表达 ML 算法。算法通过 R 类型或者 Python 类型的语法进行表达。DML 通过提供灵活的定制分析表达和独立于底层输入格式和物理数据表示的数据显著提升数据科学的生产力。
SystemML 运行环境支持 Windows、Linux 及 MacOS，可支持单机和分布式部署。单机部署显然有利于本地开发的工作，而分布式部署则可以真正发挥机器学习的威力，支持的框架包括 Hadoop 和 Spark
众所周知的IBM AIWaston融入了不少SystemML技术（不了解的同学可以看下《Jeopardy!》节目，来领教到沃森的威力）
参考：
&lt;a href=&#34;http://systemml.apache.org&#34;&gt;http://systemml.apache.org&lt;/a&gt;
&lt;a href=&#34;https://github.com/apache/incubator-systemml&#34;&gt;https://github.com/apache/incubator-systemml&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;三星：VELES
VELES 是分布式深度学习应用系统，号称：用户只需要提供参数，剩下的我来搞，VELES使用 Python 编写，使用OpenCL 或者 CUDA，利用基于Flow 的编程方式。
参考：
&lt;a href=&#34;https://velesnet.ml&#34;&gt;https://velesnet.ml&lt;/a&gt;
&lt;a href=&#34;https://github.com/Samsung/veles&#34;&gt;https://github.com/Samsung/veles&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;百度：期待ING。。。&lt;/p&gt;

&lt;p&gt;巨头之所以开源自己耗时多年打造的机器学习框架，是希望能够加速在人工智能方面的部署，在人工智能日益重要的未来抢占更多的主导权。而对于机器人创业公司来说，当这么多巨头将机器学习平台开源后，还有什么理由做不好机器人。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rosona.github.io/img/qrcode_for_beluga.jpg&#34; alt=&#34;微信公众号&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>